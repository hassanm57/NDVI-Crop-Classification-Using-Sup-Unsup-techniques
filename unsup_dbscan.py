# -*- coding: utf-8 -*-
"""unsup_dbscan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bjNNb2_785CDgRFNOdVNOL2vx1VwGaqj

Name: Hassan Mansoor

CMS: 403544

Class: BSCS12A

# **Unsupervised** **Algorithms**

**Data Preparation**

Mount Drive
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""Import The Libraries"""

import os
import pandas as pd

"""Importing The Data"""

# Define the folder paths
base_path = 'Crop-dataset'
rice_folder = os.path.join(base_path, 'Rice')
cotton_folder = os.path.join(base_path, 'Cotton')

# Load CSV files
rice_2021 = pd.read_csv(os.path.join(rice_folder, 'rice2021.csv'))
rice_2022 = pd.read_csv(os.path.join(rice_folder, 'rice2022.csv'))
rice_2023 = pd.read_csv(os.path.join(rice_folder, 'rice2023.csv'))

cotton_2021 = pd.read_csv(os.path.join(cotton_folder, 'cotton2021.csv'))
cotton_2022 = pd.read_csv(os.path.join(cotton_folder, 'cotton2022.csv'))
cotton_2023 = pd.read_csv(os.path.join(cotton_folder, 'cotton2023.csv'))

"""Add Year Column To Each Datafile"""

# Add year column to each dataset
rice_2021['year'] = 2021
rice_2022['year'] = 2022
rice_2023['year'] = 2023

cotton_2021['year'] = 2021
cotton_2022['year'] = 2022
cotton_2023['year'] = 2023

# Verify the addition
print("Rice 2021 with year column:\n", rice_2021.head())
print("Cotton 2021 with year column:\n", cotton_2021.head())

"""Add Labels To Datasset"""

# Add labels
rice_2021['label'] = 'rice'
rice_2022['label'] = 'rice'
rice_2023['label'] = 'rice'

cotton_2021['label'] = 'cotton'
cotton_2022['label'] = 'cotton'
cotton_2023['label'] = 'cotton'

print("Rice 2021 data:\n", rice_2021.head())
print("Cotton 2021 data:\n", cotton_2021.head())

"""# Combining The Datasets"""

import pandas as pd

# Combine rice datasets
rice_combined = pd.concat([rice_2021, rice_2022, rice_2023], axis=0)

# Combine cotton datasets
cotton_combined = pd.concat([cotton_2021, cotton_2022, cotton_2023], axis=0)

# Merge rice and cotton datasets
all_data = pd.concat([rice_combined, cotton_combined], axis=0).reset_index(drop=True)

# Shuffle the dataset
all_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)

print("Combined dataset:\n", all_data.head())
print("Shape of the dataset:", all_data.shape)

"""# Data Normalization And Preprocessing

# Time Series Shifting
"""

# Identify NDVI columns
ndvi_columns = [col for col in all_data.columns if 'NDVI' in col]

# Shift cotton NDVI values forward to simulate earlier growth (by 2 months)
cotton_shifted = cotton_combined.copy()
cotton_shifted[ndvi_columns] = cotton_shifted[ndvi_columns].shift(periods=2, axis=0)

# Shift rice NDVI values backward to simulate later growth (by 2 months)
rice_shifted = rice_combined.copy()
rice_shifted[ndvi_columns] = rice_shifted[ndvi_columns].shift(periods=-2, axis=0)

# Combine the shifted datasets back with the original ones (for augmentation)
augmented_data = pd.concat([cotton_shifted, rice_shifted], axis=0).reset_index(drop=True)

"""Shifting the NDVI time series helps simulate different seasonal growth cycles for rice and cotton and introduces variability into the model.
This approach will make the model more adaptable to the natural seasonality of the crops, improving its performance.

Handle Missing Values
"""

# Check for missing values
missing_values = augmented_data.isnull().sum()

# Print missing values for each column
print("Missing values for each column:\n", missing_values)

"""Fill in missing values with mean"""

# Calculate the mean of each column (excluding non-numeric columns)
column_means = augmented_data.select_dtypes(include=['number']).mean()

# Fill missing values with the calculated means
augmented_data = augmented_data.fillna(column_means)

missing_values = augmented_data.isnull().sum()
# Print missing values for each column
print("Missing values for each column:\n", missing_values)

"""No missing values anymore!

Encode The Labels (1 for Rice - 0 For Cotton)
"""

from sklearn.preprocessing import LabelEncoder

# Encode the label column
label_encoder = LabelEncoder()
augmented_data['label_encoded'] = label_encoder.fit_transform(augmented_data['label'])
augmented_data = augmented_data.drop('label', axis=1)

"""# Feature Selection"""

from sklearn.feature_selection import SelectKBest, f_classif

# Separate features (X) and target labels (y)
X = augmented_data.drop(columns=['label_encoded'])
y = augmented_data['label_encoded']

# Perform feature selection
selector = SelectKBest(score_func=f_classif, k=3)  # Select top - features. done so dbscan can run without filling up memory
X_selected = selector.fit_transform(X, y)

# Get the names of the selected features
selected_features = X.columns[selector.get_support()]
print("Selected features:", selected_features)

# Update the balanced_data with only the selected features
balanced_data = pd.DataFrame(X_selected, columns=selected_features)
balanced_data['label_encoded'] = y

"""# Apply SMOTE (Synthetic Minority Oversampling Technique)"""

# Install imbalanced-learn library
!pip install imbalanced-learn

# Import SMOTE
from imblearn.over_sampling import SMOTE

# Use only the selected features for SMOTE
X = augmented_data[selected_features]  # Use selected features
y = augmented_data['label_encoded']

# Apply SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine resampled features and labels back into a DataFrame
balanced_data = pd.DataFrame(X_resampled, columns=selected_features)  # Use selected feature names
balanced_data['label_encoded'] = y_resampled

# Check the new class distribution after SMOTE
print("Class distribution after SMOTE:\n", balanced_data['label_encoded'].value_counts())

# Show the first few rows of the resampled data
print("Resampled data:\n", balanced_data.head())

"""# Normalize The NDVI Values"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import shuffle

# Normalize the selected feature columns
scaler = MinMaxScaler()
balanced_data[selected_features] = scaler.fit_transform(balanced_data[selected_features])  # Use selected features
balanced_data = shuffle(balanced_data)
balanced_data = balanced_data.sample(frac=0.3) # done so that dbscan can run without filling up memory
print("Normalized selected feature values:\n", balanced_data[selected_features].head())

# Check unique labels and their counts
print("Unique labels in the dataset:", balanced_data['label_encoded'].unique())
print("Label distribution:\n", balanced_data['label_encoded'].value_counts())

"""# **Copy Dataframe With True Labels To Another**"""

# Create a copy of balanced_data
true_val_df = balanced_data.copy()

# Verify the copy
print(true_val_df.head())

"""# **Remove Labels For Unsupervised Algorithm**

View data columns

Remove Unnecessary Columns
"""

# Remove 'label_encoded' columns
# balanced_data_label = balanced_data.drop('label_encoded', axis=1)
# balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)
balanced_data = balanced_data.drop(['label_encoded'], axis=1)
balanced_data_0 = balanced_data.copy() # for dbscan without pca
balanced_data_0 = balanced_data_0.sample(frac=1, random_state=42).reset_index(drop=True) # shuffle the data again for randomness (same state for reproducibility)

# Verify the changes
print(balanced_data.columns)
print(balanced_data.shape)

print(balanced_data_0.columns)
print(balanced_data_0.shape)

"""# **DBSCAN**

**Without PCA**

Importing Necessary Libraries
"""

from sklearn.cluster import DBSCAN
from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay
from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import hdbscan
import matplotlib.pyplot as plt
from scipy.optimize import linear_sum_assignment

"""Initialise And Fit"""

print(balanced_data_0.shape)

from sklearn.cluster import DBSCAN
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors

# dbscan = DBSCAN(eps=0.5, min_samples=5)
# clusters = dbscan.fit_predict(balanced_data_0)

# plt.scatter(balanced_data_0.iloc[:, 0], balanced_data_0.iloc[:, 1], c=clusters, cmap='viridis', s=10)
# plt.title('DBSCAN Clustering with Pruned Small Clusters')
# plt.xlabel('Feature 1')
# plt.ylabel('Feature 2')
# plt.show()


hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=100, gen_min_span_tree=True)
clusters = hdbscan_clusterer.fit_predict(balanced_data_0)

# Plot the clusters
plt.scatter(balanced_data_0.iloc[:, 0], balanced_data_0.iloc[:, 1], c=clusters, cmap='viridis', s=10)
plt.title('HDBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

"""# **Evaluate The Cluster (Cluster Purity)**"""

def calculate_purity(y_true, y_pred):
    """
    Calculates the cluster purity by comparing the predicted clusters with the true labels.
    It measures how well the clusters match the true class labels. A higher purity value
    indicates that the clustering algorithm has done a better job at grouping similar data points together.

    Parameters:
    ----------
    y_true : array-like of shape (n_samples,)
        True class labels for each sample in the dataset.

    y_pred : array-like of shape (n_samples,)
        Predicted cluster labels for each sample, as output by the clustering algorithm.

    Returns:
    -------
    float
        The purity score, which is the proportion of correctly assigned samples in the dataset.

    """
    contingency_matrix = confusion_matrix(y_true, y_pred)
    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)


# Calculate purity for the clustering
purity = calculate_purity(true_val_df['label_encoded'], clusters)
print(f"Cluster Purity without PCA: {purity*100:.2f}")

"""# **Confusion Matrix**"""

def plot_confusion_matrix(y_true, y_pred, labels):
    """
    Plots a confusion matrix to visualize the performance of clustering.

    Parameters:
    ----------
    y_true : array-like of shape (n_samples,)
        True class labels.

    y_pred : array-like of shape (n_samples,)
        Predicted cluster labels.

    labels : list of str
        The class labels to display on the axes.

    """
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Clusters')
    plt.ylabel('True Labels')
    plt.show()

print(true_val_df['label_encoded'].unique())
clusters[clusters == -1] = 1
print(np.unique(clusters))

# # Plot the confusion matrix
plot_confusion_matrix(true_val_df['label_encoded'], clusters, labels=['Cotton', 'Rice'])
# print(confusion_matrix(true_val_df['label_encoded'], clusters))

"""# **Cluster Visualisation**"""

def visualize_clusters(data, cluster_labels, title='Cluster Visualization'):
    """
    Visualizes the clustering results by plotting the data points in a 2D space.

    Parameters:
    ----------
    data : pandas DataFrame
        The data to plot, with features for the scatter plot.

    cluster_labels : array-like of shape (n_samples,)
        The predicted cluster labels for each data point.

    title : str, optional (default='Cluster Visualization')
        The title of the plot.
    """
    plt.figure(figsize=(8, 6))
    plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis', s=10)
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

# Visualize clusters
visualize_clusters(balanced_data_0, clusters, title='DBSCAN Clustering Without PCA')

"""# **DBSCAN With PCA**

Principal Component Analysis
"""

from sklearn.decomposition import PCA # import the libraries needed
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# Apply PCA to reduce dimensions
pca = PCA(n_components=2)
X_pca = pca.fit_transform(balanced_data)
print(X_pca.shape)

"""Fit dbscan"""

# Apply DBSCAN on PCA-transformed data

dbscan = DBSCAN(eps=0.2, min_samples=3)
clusters_pca = dbscan.fit_predict(X_pca)

# hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=500, gen_min_span_tree=True)
# clusters_pca = hdbscan_clusterer.fit_predict(X_pca)

print(np.unique(clusters_pca))
clusters_pca[clusters_pca == -1] = 0
print(np.unique(clusters_pca))

"""Visualise The Clusters After Applying PCA"""

# Plotting the clusters
plt.scatter(balanced_data.iloc[:, 0], balanced_data.iloc[:, 1], c=clusters_pca, cmap='viridis', marker='o')
plt.title('DBSCAN Clustering with PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

"""Calculate Cluster Purity With PCA"""

# Calculate the cluster purity with PCA
purity_pca = calculate_purity(true_val_df['label_encoded'], clusters_pca)  # against true labels

# Print the rounded purity value
print(f"Cluster Purity with PCA: {round(purity_pca * 100, 1)}%")

"""Plot The Confusion Matrix"""

# Plot the confusion matrix for DBSCAN with PCA
plot_confusion_matrix(true_val_df['label_encoded'], clusters_pca, labels=['Cotton', 'Rice'])
print(confusion_matrix(true_val_df['label_encoded'], clusters_pca))