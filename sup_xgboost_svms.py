# -*- coding: utf-8 -*-
"""sup_xgboost_svms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pWW-d8I0XMncbqvhyF02lcTMdqlBXoCD
"""

# basic imports
import pandas as pd
import os
import numpy as np

# sklearn imports
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import shuffle
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.feature_selection import RFE
from sklearn.inspection import permutation_importance

# sklearn genetic algo imports
from sklearn_genetic import GAFeatureSelectionCV, GASearchCV
from sklearn_genetic.callbacks import LogbookSaver
from sklearn_genetic.space import Continuous, Categorical, Integer
from sklearn_genetic.callbacks import LogbookSaver

from scipy.stats import uniform, randint



import xgboost as xgb

import seaborn as sns

import matplotlib.pyplot as plt

from imblearn.over_sampling import SMOTE

from utils.input_data import run_augmentation

import time

import itertools

import tqdm

"""# All the necessary functions"""

def load_data_from_csv(cotton_datset_path, rice_datset_path):
    """
    Args:
        cotton_datset_path (str): The file path to the directory containing cotton dataset CSV files.
        rice_datset_path (str): The file path to the directory containing rice dataset CSV files.

    Description:
        This function loads CSV files from the specified directories for cotton and rice datasets.
        Each CSV file is read into a pandas DataFrame, and a 'label' column is added to indicate
        the class (0 for cotton, 1 for rice). The function returns two lists of DataFrames, one
        for cotton and one for rice.

    Returns:
        cotton_datset (list of pd.DataFrame): A list of DataFrames containing cotton data with a 'label' column.
        rice_datset (list of pd.DataFrame): A list of DataFrames containing rice data with a 'label' column.
    """
    cotton_datset, rice_datset = [], []

    for file in sorted(os.listdir(cotton_datset_path)):
        temp_df = pd.read_csv(os.path.join(cotton_datset_path, file))
        temp_df['label'] = 0
        cotton_datset.append(temp_df)


    for file in sorted(os.listdir(rice_datset_path)):
        temp_df = pd.read_csv(os.path.join(rice_datset_path, file))
        temp_df['label'] = 1
        rice_datset.append(temp_df)

    return cotton_datset, rice_datset

def iqr_removal(df): # assumes the last column in df.columns is the label and does not include it in the IQW removal
    print(f'original shape: {df.shape}')

    Q1 = df[df.columns[:-1]].quantile(0.25)
    Q3 = df[df.columns[:-1]].quantile(0.75)

    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5*IQR
    upper_bound = Q3 + 1.5*IQR

    # ~ is the bitwise NOT operator
    outliers = df[~~((df[df.columns[:-1]] < (lower_bound)) |(df[df.columns[:-1]] > (upper_bound))).any(axis=1)]

    df = df.drop(outliers.index)
    print(f'new shape: {df.shape}')
    return df


def three_year_plots(cotton_data, rice_data, sample_size=1000, average=True, num_years=3, data_normalisation='minmax', remove_outliers=False):

    years = list(range(num_years))

    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))

    for idx, year in enumerate(years):
        cotton_data_year = [cotton_data[year]]
        rice_data_year = [rice_data[year]]

        print(f'year: {year+2077}')
        if remove_outliers:
            cotton_resampled_df = iqr_removal(pd.concat(cotton_data_year))
            rice_resampled_df = iqr_removal(pd.concat(rice_data_year))
        else:
            cotton_resampled_df = cotton_data_year[0]
            rice_resampled_df = rice_data_year[0]

        cotton_resampled_df, rice_resampled_df = standardize(cotton_resampled_df, rice_resampled_df, 'minmax')
        sample_size = sample_size

        plot_ndvi_subset(cotton_resampled_df, rice_resampled_df,
                        sample_size=sample_size, plot_average=average, ax=axes[idx])

        axes[idx].set_title(f'Year {year}')

    plt.tight_layout()
    plt.show()


def standardize(cotton_data, rice_data, data_normalisation = 'minmax'):
    """
    takes in two dataframes of two list of dataframes (in which case they are pd.concatenated)
    then, they are minmaxed or standardised
    and then, two pd dataframes cotton_df and rice_df are returned

    """
    if type(cotton_data) == list and type(rice_data) == list:
        concat_cotton_df, concat_rice_df = pd.concat(cotton_data), pd.concat(rice_data) # axis 0 by default
    elif type(cotton_data) == pd.DataFrame and type(rice_data) == pd.DataFrame:
        concat_cotton_df, concat_rice_df = cotton_data, rice_data


    columns = concat_cotton_df.columns.to_list()[:-1]

    X_cotton, y_cotton = concat_cotton_df.drop('label', axis=1), concat_cotton_df['label']
    X_rice, y_rice = concat_rice_df.drop('label', axis=1), concat_rice_df['label']

    y_cotton = np.array(y_cotton) # to reset the index
    y_rice = np.array(y_rice) # to reset the index



    if data_normalisation == 'nonorm':
        return concat_cotton_df, concat_rice_df

    if data_normalisation == 'standard':
        scaler = StandardScaler()
    elif data_normalisation == 'minmax':
        scaler = MinMaxScaler()
    else:
        raise ValueError('Choose a valid data normalisation method')

    X_cotton = np.array(scaler.fit_transform(X_cotton))
    X_rice = np.array(scaler.fit_transform(X_rice))

    cotton_df = pd.DataFrame(X_cotton, columns=columns)
    cotton_df['label'] = y_cotton

    rice_df = pd.DataFrame(X_rice, columns=columns)
    rice_df['label'] = y_rice

    return shuffle(cotton_df), shuffle(rice_df)

def balance_classes(cotton_data, rice_data):
    """
    Args:
        cotton_data (pd.DataFrame): A ilst of DataFrame containing cotton data with a 'label' column.
        rice_data (pd.DataFrame): A list of DataFrame containing rice data with a 'label' column.
        data_normalisation (str): The type of data normalisation to apply ('standard', 'minmax', or None).

    Description:
        This function balances the classes in the cotton and rice datasets using SMOTE (Synthetic Minority
        Over-sampling Technique). It concatenates the cotton and rice DataFrames, resamples the data to
        balance the classes, and optionally applies data normalisation. The function returns the resampled
        DataFrames for cotton and rice separately.

    Returns:
        cotton_resampled_df (pd.DataFrame): A DataFrame containing balanced cotton data with normalised features.
        rice_resampled_df (pd.DataFrame): A DataFrame containing balanced rice data with normalised features.
    """
    if type(cotton_data) == list and type(rice_data) == list:
        concat_cotton_df, concat_rice_df = pd.concat(cotton_data), pd.concat(rice_data)

    else:
        concat_cotton_df, concat_rice_df = cotton_data, rice_data

    combined_df = pd.concat([concat_cotton_df, concat_rice_df])
    X, y = combined_df.drop('label', axis=1), combined_df['label']

    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)

    resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
    resampled_df['label'] = np.array(y_resampled)

    cotton_resampled_df = resampled_df[resampled_df['label'] == 0]
    rice_resampled_df = resampled_df[resampled_df['label'] == 1]

    return cotton_resampled_df, rice_resampled_df

def plot_ndvi_subset(cotton_df, rice_df, sample_size=100, plot_average=False, ax=None):
    ndvi_cols = [f'NDVI{i:02}' for i in range(1, 13)]
    julian_days = []
    for i in range(12): # put range = 13 for 12 intervals but it causes problems, so set range 12 and divide by 11
        julian_days.append(121 + int(i*214/11))


    if ax is None:
        plt.figure(figsize=(12, 6))
        ax = plt.gca()  # Get the current axis


    if plot_average:
        cotton_avg = cotton_df[ndvi_cols].mean()
        rice_avg = rice_df[ndvi_cols].mean()

        ax.plot(julian_days, cotton_avg.values, color='blue', label='Cotton Average')
        ax.plot(julian_days, rice_avg.values, color='green', label='Rice Average')
    else:
        # cotton_ndvi = cotton_df[ndvi_cols + ['label']].sample(n=min(sample_size, cotton_df.shape[0]-1, rice_df.shape[0]-1), random_state=42)
        # rice_ndvi = rice_df[ndvi_cols + ['label']].sample(n=min(sample_size, cotton_df.shape[0]-1, rice_df.shape[0]-1), random_state=42)

        cotton_ndvi = cotton_df[ndvi_cols + ['label']].sample(n=min(sample_size, cotton_df.shape[0]-1), random_state=42)
        rice_ndvi = rice_df[ndvi_cols + ['label']].sample(n=min(sample_size, rice_df.shape[0]-1), random_state=42)

        cotton_ndvi['Crop'] = 'Cotton'
        rice_ndvi['Crop'] = 'Rice'

        combined_df = pd.concat([cotton_ndvi, rice_ndvi])
        combined_df.set_index('label', inplace=True)

        for crop in combined_df['Crop'].unique():
            subset = combined_df[combined_df['Crop'] == crop]
            for idx, row in subset.iterrows():
                ax.plot(julian_days, row[ndvi_cols],
                         color='blue' if crop == 'Cotton' else 'green',
                         alpha=0.1)

    ax.set_xlim(0, 365)
    ax.set_xticks(range(0, 366, 50))

    ax.set_title('NDVI Values Over Time for Cotton and Rice')
    ax.set_xlabel('Julian Day')
    ax.set_ylabel('NDVI Value')
    ax.grid(True, which='both', linestyle='--', linewidth=0.5)

    if plot_average:
        ax.legend(title='Crop Type')
    else:
        ax.legend(handles=[plt.Line2D([0], [0], color='blue', label='Cotton'),
                            plt.Line2D([0], [0], color='green', label='Rice')],
                   title='Crop Type')

def train_model(X_train, y_train, ml_algo='xgboost', cv=5, n_jobs=-1):
    """
    Args:
        X_train (pd.DataFrame or np.ndarray): The feature matrix for training.
        y_train (pd.Series or np.ndarray): The target labels for training.
        ml_algo (str): The machine learning algorithm to use ('xgboost', 'svc', or 'random_forest').
        cv (int): The number of cross-validation folds to use.
        n_jobs (int): The number of CPU cores to use for parallel processing.

    Description:
        This function trains a machine learning model using the specified algorithm. The available
        options are 'xgboost', 'svc', and 'random_forest'. The function fits the model on the training
        data and optionally performs cross-validation to evaluate the model's performance.

    Returns:
        model: The trained machine learning model.
    """
    if ml_algo == 'xgboost':
        model = xgb.XGBClassifier(eval_metric= 'logloss', learning_rate=1.0, n_estimators=95, max_depth=2)
        # model = xgb.XGBClassifier(eval_metric= 'logloss')
        model.fit(X_train, y_train)

    elif ml_algo == 'svc':
        model = SVC(cache_size= 4000, kernel='rbf', class_weight='balanced', C=0.1, gamma='scale')
        # model = SVC(kernel='rbf')
        model.fit(X_train, y_train)

    elif ml_algo == 'random_forest':
        model = RandomForestClassifier(criterion='log_loss', n_estimators=15, max_depth=5, min_samples_split=15)
        # model = RandomForestClassifier(criterion='log_loss')
        model.fit(X_train, y_train)

    elif ml_algo == 'bagging':
        base_estimator = DecisionTreeClassifier()
        model = BaggingClassifier(estimator=base_estimator, n_estimators=165,max_samples=2,max_features=4)
        # model = BaggingClassifier(estimator=base_estimator)
        model.fit(X_train, y_train)
    else:
        raise ValueError('Invalid \'ml_algo\'. Choose a valid model ')

    # carry out cross validation
    # cval_score = cross_val_score(model, X_train, y_train, cv=cv, n_jobs=n_jobs)
    # print(f'Cross validation score: {cval_score.mean()}')
    return model


def evaluate_model(X_test, y_test, model):
    """
    Args:
        X_test (pd.DataFrame or np.ndarray): The feature matrix for testing.
        y_test (pd.Series or np.ndarray): The target labels for testing.
        model: The trained machine learning model to evaluate.

    Description:
        This function evaluates the performance of a trained model on the test dataset. It computes
        the accuracy score and provides a detailed classification report, including precision, recall,
        and F1-score for each class.

    Returns:
        None: The function prints the evaluation metrics.
    """
    y_pred = model.predict(X_test)
    print(accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))


def split_df(df, test_size=0.2):
    """
    Args:
        df (pd.DataFrame): The input DataFrame containing features and a 'label' column.

    Description:
        This function splits the input DataFrame into training and testing sets. The 'label' column
        is used as the target variable, and the remaining columns are treated as features. The data
        is split into 80% training and 20% testing, with a fixed random state for reproducibility.

    Returns:
        X_train (pd.DataFrame or np.ndarray): The feature matrix for training.
        X_test (pd.DataFrame or np.ndarray): The feature matrix for testing.
        y_train (pd.Series or np.ndarray): The target labels for training.
        y_test (pd.Series or np.ndarray): The target labels for testing.
    """
    X = df.drop('label', axis=1)
    y = df['label']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    return X_train, X_test, y_train, y_test


def plot_confmatrix(X_test, y_test, model):
    """
    Args:
        X_test (pd.DataFrame or np.ndarray): The feature matrix for testing.
        y_test (pd.Series or np.ndarray): The target labels for testing.
        model: The trained machine learning model to evaluate.

    Description:
        This function generates and plots a confusion matrix for the model's predictions on the test
        dataset. The confusion matrix visualizes the true vs. predicted labels, helping to assess the
        model's performance in terms of false positives, false negatives, true positives, and true negatives.

    Returns:
        None: The function displays the confusion matrix plot.
    """
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, fmt='d', annot=True)
    plt.show()

class Args:
    def __init__(self, **kwargs ):

        self.gpus = kwargs.get('gpus')
        self.dataset = kwargs.get('dataset')
        self.model = kwargs.get('model')
        self.train = kwargs.get('train')
        self.save = kwargs.get('save')
        self.augmentation_ratio = kwargs.get('augmentation_ratio')
        self.seed = kwargs.get('seed')
        self.jitter = kwargs.get('jitter')
        self.scaling = kwargs.get('scaling')
        self.permutation = kwargs.get('permutation')
        self.randompermutation = kwargs.get('randompermutation')
        self.magwarp = kwargs.get('magwarp')
        self.timewarp = kwargs.get('timewarp')
        self.windowslice = kwargs.get('windowslice')
        self.windowwarp = kwargs.get('windowwarp')
        self.rotation = kwargs.get('rotation')
        self.spawner = kwargs.get('spawner')
        self.dtwwarp = kwargs.get('dtwwarp')
        self.shapedtwwarp = kwargs.get('shapedtwwarp')
        self.wdba = kwargs.get('wdba')
        self.discdtw = kwargs.get('discdtw')
        self.discsdtw = kwargs.get('discsdtw')
        self.extra_tag = kwargs.get('extra_tag')
        self.preset_files = kwargs.get('preset_files')
        self.ucr = kwargs.get('ucr')
        self.ucr2018 = kwargs.get('ucr2018')
        self.data_dir = kwargs.get('data_dir')
        self.train_data_file = kwargs.get('train_data_file')
        self.train_labels_file = kwargs.get('train_labels_file')
        self.test_data_file = kwargs.get('test_data_file')
        self.test_labels_file = kwargs.get('test_labels_file')
        self.test_split = kwargs.get('test_split')
        self.weight_dir = kwargs.get('weight_dir')
        self.log_dir = kwargs.get('log_dir')
        self.output_dir = kwargs.get('output_dir')
        self.normalize_input = kwargs.get('normalize_input')
        self.delimiter = kwargs.get('delimiter')
        self.optimizer = kwargs.get('optimizer')
        self.lr = kwargs.get('lr')
        self.validation_split = kwargs.get('validation_split')
        self.iterations = kwargs.get('iterations')
        self.batch_size = kwargs.get('batch_size')
        self.verbose = kwargs.get('verbose')


def augment(kwargs, cotton_df, rice_df, gclass_upsample_ratio=1):
    assert type(gclass_upsample_ratio) == int
    # augmentation started ############################################################
    print('Augmenting data')

    column_names = cotton_df.columns.to_list()[:-1]


    args = Args(**kwargs)


    print(f'before augmentation cotton_df.shape: {cotton_df.shape}   rice_df.shape: {rice_df.shape} ')

    # we will augment cotton data to gclass_upsampled times its is original size
    if gclass_upsample_ratio > 1:
        args.augmentation_ratio = gclass_upsample_ratio # the ratio by which the class with more samples get upsampled
        X, y = cotton_df.drop('label', axis=1), cotton_df['label']
        X, y = np.array(X), np.array(y)
        X = np.reshape(X, (X.shape[0], X.shape[1], 1))
        X, y, augmentation_tags = run_augmentation(X, y, args)
        X = np.reshape(X, (X.shape[0], X.shape[1]))
        cotton_df = pd.DataFrame(X, columns=column_names)
        cotton_df['label']= y



    # ratio will depend on how less the rice data is compared to cotton data
    ratio = (len(cotton_df) // len(rice_df))
    args.augmentation_ratio = ratio

    # since we know that the rice data is less than cotton data, we'll augment the rice data
    X, y = rice_df.drop('label', axis=1), rice_df['label']
    X, y = np.array(X), np.array(y)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    X, y, augmentation_tags = run_augmentation(X, y, args)
    X = np.reshape(X, (X.shape[0], X.shape[1]))
    rice_df = pd.DataFrame(X, columns=column_names)
    rice_df['label']= y
    # y_train = np.reshape(y_train, (y_train.shape[0], y_train.shape[1], 1))


    print(f'augmentation_tags: {augmentation_tags}')
    print(f'after augmentation cotton_df.shape: {cotton_df.shape}   rice_df.shape: {rice_df.shape} ')





    # augmentation done ############################################################
    return shuffle(cotton_df), shuffle(rice_df), augmentation_tags

def ty_cross_val(cotton_data, rice_data, ml_algo, should_augment=True, kwargs=None, gclass_upsample_ratio=1):

    # cotton_data and rice data are a list of pdf dataframes
    assert len(cotton_data) == len(rice_data), 'The number of cotton and rice dataframes must be equal'

    # num_years = 3
    # macro_precision = 0
    # macro_recall = 0
    # macro_avg = 0

    # total_samples = sum(  [  ( len(cotton_data[i])+len(rice_data[i])  ) for i in range(num_years)  ]   )
    # print(f'total_samples: {total_samples}')

    acc_avg = 0
    final_matrix = [[0,0], [0,0]]
    for i in range(3):
        cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)

        column_names = cotton_data[i].columns.to_list()[:-1]

        val_cotton, val_rice = [cotton_data[i], rice_data[i]]

        cotton_2y_train_list, rice_2y_train_list = cotton_data[:i] + cotton_data[i+1:], rice_data[:i] + rice_data[i+1:]
        cotton_resampled_df = pd.concat(cotton_2y_train_list)
        rice_resampled_df = pd.concat(rice_2y_train_list)

        # cotton_resampled_df, rice_resampled_df = balance_classes(cotton_2y_train_list, rice_2y_train_list)

        augmentation_tags = None
        if should_augment:
            cotton_resampled_df, rice_resampled_df, augmentation_tags = augment(kwargs, cotton_resampled_df, rice_resampled_df, gclass_upsample_ratio)
        if augmentation_tags == None:
            augmentation_tags = 'no_aug'
        print(augmentation_tags)


        cotton_resampled_df, rice_resampled_df = standardize(cotton_resampled_df, rice_resampled_df, 'minmax')

        print(f'cotton_resampled_df.shape: {cotton_resampled_df.shape}, rice_resampled_df.shape: {rice_resampled_df.shape}')
        print(f'val_data cotton_val.shape: {cotton_data[0].shape}  rice_val.shape: {rice_data[0].shape}')



        combined_df = pd.concat([cotton_resampled_df, rice_resampled_df])
        print(combined_df['label'].value_counts())
        combined_df = iqr_removal(combined_df)
        print(combined_df['label'].value_counts())
        X_train, y_train = combined_df.drop('label', axis=1),combined_df['label']


        # val_data = iqr_removal(val_data)
        val_cotton_df, val_rice_df = standardize(val_cotton, val_rice, 'minmax')
        val_data = pd.concat([val_cotton_df, val_rice_df])

        X_test, y_test = val_data.drop('label', axis=1), val_data['label']

        print(type(X_test))
        print(X_test.shape)

        X_test = pd.DataFrame(X_test, columns=column_names)


        model = train_model(X_train, y_train, ml_algo=ml_algo)

        y_pred = model.predict(X_test)
        matrix = confusion_matrix(y_test, y_pred)
        final_matrix += matrix
        acc_cotton, acc_rice = matrix.diagonal()/matrix.sum(axis=1)
        acc_temp = (acc_cotton + acc_rice)/2
        acc_avg += acc_temp/3

        print(f'precision_score: {precision_score(y_test, y_pred):.4f}')
        print(f'recall_score: {recall_score(y_test, y_pred):.4f}')
        print(f'f1 score: {f1_score(y_test, y_pred):.4f}')
        print(f'accuracy_score: {accuracy_score(y_test, y_pred):.4f}')

        # b_accuracy_score = balanced_accuracy_score(y_test, y_pred)
        # print(f'balanced_accuracy_score: {b_accuracy_score:.4f}')
        # evaluate_model(X_test, y_test, model)

        print()
        print('***********************************************************************************************')
        print('***********************************************************************************************')
        print()
    print(f'final_matrix: {final_matrix}')
    print(f'Balanced  acc: {acc_avg}')

    return final_matrix, acc_avg
def give_diff_from_mean(df):
    mean_vals = df.mean()[:-1]
    df = df.apply(lambda x: pd.concat([x[:-1]-mean_vals, pd.DataFrame(x[-1], columns='label')]), axis=1)
    return df

kwargs = {
        'gpus': "",
        'dataset': "custom rice cotton dataset",
        'model': "no model"  ,
        'train': False  ,
        'save': False  ,
        'seed': 2  ,


        'augmentation_ratio': 1 ,
        'jitter': False ,
        'scaling': False ,
        'permutation': False,
        'randompermutation': False ,
        'magwarp': False ,
        'timewarp': False ,
        'windowslice': False ,
        'windowwarp': True ,
        'rotation': False ,
        'spawner': False ,
        'dtwwarp': False ,
        'shapedtwwarp': False ,
        'wdba': False ,
        'discdtw': False ,
        'discsdtw': False,

        'extra_tag': "",
        'preset_files': False  ,
        'ucr': False  ,
        'ucr2018': False  ,
        'data_dir': "" ,
        'train_data_file': "",
        'train_labels_file': "",
        'test_data_file': "",
        'test_labels_file': "",
        'test_split': 0  ,
        'weight_dir': ""  ,
        'log_dir': ""  ,
        'output_dir': ""  ,
        'normalize_input': False  ,
        'delimiter': ""  ,
        'optimizer': ""  ,
        'lr': 1e-2,
        'validation_split': 0  ,
        'iterations': 10000  ,
        'batch_size': 256  ,
        'verbose': 1  ,
    }
aug_list = []
for i, key in enumerate(kwargs):
    if i >=7 and i<= 21:
        aug_list.append(key)
cotton_datset_path = 'Crop-dataset/Cotton'
rice_datset_path = 'Crop-dataset/Rice'


cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
cotton_resampled_df, rice_resampled_df = balance_classes(cotton_data, rice_data) # applies SMOTE
cotton_resampled_df, rice_resampled_df = standardize(cotton_resampled_df, rice_resampled_df, 'minmax')
combined_df = pd.concat([cotton_resampled_df, rice_resampled_df])


print(cotton_resampled_df.shape, rice_resampled_df.shape)
print(combined_df.shape)

"""# Data Analysis

### This is how clean NDVI values look
#### For rice and cotton, we see defined areas where the maximum value occur. This is the expected behavior for these crops.
#### On the left, we have normalised NDVI values from the MO-08 dataset
#### In the middle, we have NDVI values for cotton in Pakistan from MODIS dataset by USDA (US department of agriculture)
#### And on right, we have NDVI values for rice in Pakistan from MODIS dataset



<img src="images/mo08_ndvi_data.png" style="height:500px; width:auto;">
<img src="images/cotton_pakistan_modis.png" style="height:500px; width:auto;">
<img src="images/rice_pakistan_modis.png" style="height:500px; width:auto;">

### This is how the original NDVI values look (after normalisation and outlier removal with IQR)
### Averaged NDVI values
"""

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)

three_year_plots(cotton_data, rice_data, sample_size=1000, average=True, num_years=3, data_normalisation='minmax', remove_outliers=False)

"""### Individual NDVI values"""

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)

three_year_plots(cotton_data, rice_data, sample_size=1000, average=False, num_years=3, data_normalisation='minmax', remove_outliers=False)

"""## Using grid search for best params"""

def use_grid_search(kwargs,ml_algo='xgboost'):

    # we'll load the data two times. one as it is for the testing
    # and anohter training.
    # this is done so that data loading is done only once
    # training data is in the format year 12, 02, 01 and the other is the val data

    _2y_training_list_3 = []
    _1y_val_list_3 = []
    for i in range(3):
        # for val load, standardize and append
        # for train, load, augment, outlier removal, standardization and append

        cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path) # reloading done just in case

        val_cotton, val_rice = [cotton_data[i], rice_data[i]]  #
        val_cotton, val_rice = standardize(val_cotton, val_rice, 'minmax')
        val_combined_df = shuffle(pd.concat([val_cotton, val_rice]))
        X_test, y_test = val_combined_df.drop('label', axis=1), val_combined_df['label']
        _1y_val_list_3.append([X_test, y_test])

        # two years worth of training data here
        train_cotton_df, train_rice_df = pd.concat(cotton_data[:i] + cotton_data[i+1:]), pd.concat(rice_data[:i] + rice_data[i+1:])
        train_cotton_df, train_rice_df, augmentation_tags = augment(kwargs, train_cotton_df, train_rice_df, gclass_upsample_ratio=1)
        train_cotton_df, train_rice_df = iqr_removal(train_cotton_df), iqr_removal(train_rice_df)
        train_cotton_df, train_rice_df = standardize(train_cotton_df, train_rice_df, 'minmax')
        train_combined_df = shuffle(pd.concat([train_cotton_df, train_rice_df]))

        # train_combined_df = give_diff_from_mean(train_combined_df) # experimental step
        # reduce training data for faster training
        # train_combined_df = train_combined_df.sample(len(train_combined_df)//10)
        # train_combined_df = train_combined_df.sample(min(10,000, len(train_combined_df)-1))

        X_train, y_train = train_combined_df.drop('label', axis=1), train_combined_df['label']
        _2y_training_list_3.append([X_train, y_train])


    # combined_df = pd.concat([pd.concat(cotton_data), pd.concat(rice_data)])
    # X_train, X_test, y_train, y_test = split_df(combined_df)

    print(f'len(_2y_training_list_3): {len(_2y_training_list_3)}')
    if ml_algo == 'random_forest':
        param_grid = {
            'n_estimators': list(np.arange(15, 180, 15)),
            'max_depth': list(np.arange(5, 16, 1)),
            'min_samples_split': list(np.arange(5, 16, 1)),
        }
    elif ml_algo == 'xgboost':
        param_grid = {
            'learning_rate': [0.1, 0.5, 0.8, 1.0, 1.5, 2.0, 4.0 ],
            'n_estimators': list(np.arange(5, 150, 15)),
            'max_depth': list(np.arange(1, 12, 1)),

        }
    elif ml_algo == 'svc':
        param_grid = {
            'kernel': ['linear', 'rbf'],
            'class_weight': ['balanced', None],
            'C': [0.00001, 0.001, 0.01, 0.1, 1.0, 5.0, 10.0],
            'gamma': ['auto', 'scale'],
        }
    elif ml_algo == 'bagging':
        param_grid = {
            'n_estimators': list(np.arange(50, 200, 5)),
            'max_samples': list(map(int, np.arange(1.0, 5.0, 1.0))),
            'max_features': list(map(int, np.arange(1.0, 5.0, 1.0))),
        }
    all_combs = list(itertools.product(*(param_grid[key] for key in param_grid)))
    print(all_combs)

    # le = LabelEncoder()
    max_acc_final = 0
    final_params = []
    # all_combs = [[10, 10, 5]] # temp
    for combo in tqdm.tqdm(all_combs): # for every combination, you'll train say xgboost three times, for each combination of years, and validate on the other
        acc_one_combo = 0
        for i in range(3): # num years to do cross validation

            if ml_algo == 'random_forest':
                # model = RandomForestClassifier(criterion='log_loss')
                model = RandomForestClassifier(criterion='log_loss', n_estimators=combo[0], max_depth=combo[1], min_samples_split=combo[2])

            elif ml_algo == 'xgboost':
                model = xgb.XGBClassifier(eval_metric= 'logloss', learning_rate=combo[0], n_estimators=combo[1], max_depth=combo[2])

            elif ml_algo == 'svc':
                model = SVC(cache_size= 4000, kernel=combo[0], class_weight=combo[1], C=combo[2], gamma=combo[3])

            elif ml_algo == 'bagging':
                base_estimator = DecisionTreeClassifier()
                model = BaggingClassifier(estimator=base_estimator,
                                        n_estimators=combo[0],
                                        max_samples=combo[1],
                                        max_features=combo[2])
            else:
                raise ValueError('Invalid \'ml_algo\'. Choose a valid model ')

            # print(f'this is i: {i}')
            X_train, y_train = _2y_training_list_3[i]
            X_test, y_test = _1y_val_list_3[i]

            # print(X_train.shape, y_train.shape)
            # print(y_train.value.label)
            model.fit(X_train, y_train)

            y_pred = model.predict(X_test)

            # return model, y_pred, y_test

            # matrix = confusion_matrix(y_test, y_pred)
            # acc_cotton, acc_rice = matrix.diagonal()/matrix.sum(axis=1)
            # acc_avg = (acc_cotton + acc_rice)/2
            # acc_one_combo += acc_avg/3

            acc_one_combo += f1_score(y_test, y_pred)/3

        if acc_one_combo > max_acc_final:
            max_acc_final = acc_one_combo
            final_params = combo
            print(f'best params till now: {final_params} acc: {acc_one_combo}')
    return final_params
kwargs = {
        'gpus': "",
        'dataset': "custom rice cotton dataset",
        'model': "no model"  ,
        'train': False  ,
        'save': False  ,
        'seed': 2  ,


        'augmentation_ratio': 1 ,
        'jitter': False ,
        'scaling': False ,
        'permutation': False,
        'randompermutation': False ,
        'magwarp': False ,
        'timewarp': False ,
        'windowslice': False ,
        'windowwarp': True ,
        'rotation': False ,
        'spawner': False ,
        'dtwwarp': False ,
        'shapedtwwarp': False ,
        'wdba': False ,
        'discdtw': False ,
        'discsdtw': False,

        'extra_tag': "",
        'preset_files': False  ,
        'ucr': False  ,
        'ucr2018': False  ,
        'data_dir': "" ,
        'train_data_file': "",
        'train_labels_file': "",
        'test_data_file': "",
        'test_labels_file': "",
        'test_split': 0  ,
        'weight_dir': ""  ,
        'log_dir': ""  ,
        'output_dir': ""  ,
        'normalize_input': False  ,
        'delimiter': ""  ,
        'optimizer': ""  ,
        'lr': 1e-2,
        'validation_split': 0  ,
        'iterations': 10000  ,
        'batch_size': 256  ,
        'verbose': 1  ,
    }


combo = use_grid_search(kwargs, 'random_forest') # cotton and rice data will be loaded inside the function

# equal accuracy preference
# random forset: (15, 5, 15)
# xgboost: (1.0, 95, 2)
# svc: ('rbf', 'balanced', 0.1, 'scale')
# bagging:  (165, 2, 4) # 66.53

# using f1 score
# random forset: (15, 11, 9)
# xgboost: (4.0, 5, 2)
# svc:
# bagging: (195, 3, 3)
print(combo)

combo = use_grid_search(kwargs, 'random_forest') # cotton and rice data will be loaded inside the function
print(combo)

combo = use_grid_search(kwargs, 'bagging') # cotton and rice data will be loaded inside the function
print(combo)

combo = use_grid_search(kwargs, 'svc') # cotton and rice data will be loaded inside the function
print(combo)

combo = use_grid_search(kwargs, 'xgboost') # cotton and rice data will be loaded inside the function
print(combo)

"""## Using hyperparameter evolution as an alternative to grid search"""

def ga_hp_evolution(X_train, y_train, X_test, y_test, ml_algo='random_forest', param_grid=None):
    if param_grid is None:
        param_grid = {
            "n_estimators": Integer(1, 200),
            "max_depth": Integer(1, 30),
            "criterion": Categorical(["gini", "entropy", "log_loss"]),
            "min_samples_split": Integer(2, 20),  # Changed from 1 to 2
            "min_samples_leaf": Integer(1, 20),
        }

    if ml_algo == 'xgboost':
        pass
    elif ml_algo == 'svc':
        pass
    elif ml_algo == 'random_forest':
        model = RandomForestClassifier()

        ga_search = GASearchCV(
            estimator=model,
            cv=5,
            population_size=20,  # Increased population size
            generations=7,       # Increased generations
            tournament_size=5,
            elitism=True,
            param_grid=param_grid,
            scoring="balanced_accuracy",  # Changed scoring metric
            n_jobs=-1,
            verbose=True,
        )
        ga_search.fit(X_train, y_train)
        print(ga_search.best_estimator_)
        model = ga_search.best_estimator_
    else:
        raise ValueError('Invalid ml_algo. Choose a valid model ')
    return model

# Example usage
X_train, X_test, y_train, y_test = split_df(combined_df)

model = ga_hp_evolution(X_train, y_train, X_test, y_test, ml_algo='random_forest')

"""## 3 year cros val function"""

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
final_matrix, acc_avg = ty_cross_val(cotton_data, rice_data, ml_algo='xgboost', should_augment=True, kwargs=kwargs, gclass_upsample_ratio=2)

plt.figure(figsize=(10, 8))
sns.heatmap(final_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix - XGBoost Model\nAccuracy: {:.4f}'.format(acc_avg))
plt.show()

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
final_matrix, acc_avg = ty_cross_val(cotton_data, rice_data, ml_algo='bagging', should_augment=True, kwargs=kwargs, gclass_upsample_ratio=2)


plt.figure(figsize=(10, 8))
sns.heatmap(final_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix - Bagging Model\nAccuracy: {:.4f}'.format(acc_avg))
plt.show()

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
final_matrix, acc_avg = ty_cross_val(cotton_data, rice_data, ml_algo='random_forest', should_augment=True, kwargs=kwargs, gclass_upsample_ratio=2)

plt.figure(figsize=(10, 8))
sns.heatmap(final_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix - Random Forest Model\nAccuracy: {:.4f}'.format(acc_avg))
plt.show()

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
final_matrix, acc_avg = ty_cross_val(cotton_data, rice_data, ml_algo='svc', should_augment=True, kwargs=kwargs, gclass_upsample_ratio=2)

plt.figure(figsize=(10, 8))
sns.heatmap(final_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix - SVM Model\nAccuracy: {:.4f}'.format(acc_avg))
plt.show()

"""# Feature Importance for the supervised learning algorithms"""

kwargs = {
        'gpus': "",
        'dataset': "custom rice cotton dataset",
        'model': "no model"  ,
        'train': False  ,
        'save': False  ,
        'seed': 2  ,


        'augmentation_ratio': 1 ,
        'jitter': False ,
        'scaling': False ,
        'permutation': False,
        'randompermutation': False ,
        'magwarp': False ,
        'timewarp': False ,
        'windowslice': False ,
        'windowwarp': True ,
        'rotation': False ,
        'spawner': False ,
        'dtwwarp': False ,
        'shapedtwwarp': False ,
        'wdba': False ,
        'discdtw': False ,
        'discsdtw': False,

        'extra_tag': "",
        'preset_files': False  ,
        'ucr': False  ,
        'ucr2018': False  ,
        'data_dir': "" ,
        'train_data_file': "",
        'train_labels_file': "",
        'test_data_file': "",
        'test_labels_file': "",
        'test_split': 0  ,
        'weight_dir': ""  ,
        'log_dir': ""  ,
        'output_dir': ""  ,
        'normalize_input': False  ,
        'delimiter': ""  ,
        'optimizer': ""  ,
        'lr': 1e-2,
        'validation_split': 0  ,
        'iterations': 10000  ,
        'batch_size': 256  ,
        'verbose': 1  ,
    }

"""## Random Forest"""

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
cotton_resampled_df, rice_resampled_df = pd.concat(cotton_data), pd.concat(rice_data)
cotton_resampled_df, rice_resampled_df, augmentation_tags = augment(kwargs, cotton_resampled_df, rice_resampled_df, gclass_upsample_ratio=2)
cotton_resampled_df, rice_resampled_df = standardize(cotton_resampled_df, rice_resampled_df, 'minmax')
cotton_resampled_df, rice_resampled_df = iqr_removal(cotton_resampled_df), iqr_removal(rice_resampled_df)
combined_df = pd.concat([cotton_resampled_df, rice_resampled_df])
combined_df = shuffle(combined_df)
X, y = combined_df.drop('label', axis=1), combined_df['label']
model = train_model(X, y, ml_algo='random_forest')

importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.barh(X.columns[indices], importances[indices])
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Feature Importance from Random Forest")
plt.show()

"""## Bagging"""

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
cotton_resampled_df, rice_resampled_df = pd.concat(cotton_data), pd.concat(rice_data)
cotton_resampled_df, rice_resampled_df, augmentation_tags = augment(kwargs, cotton_resampled_df, rice_resampled_df, gclass_upsample_ratio=2)
cotton_resampled_df, rice_resampled_df = standardize(cotton_resampled_df, rice_resampled_df, 'minmax')
cotton_resampled_df, rice_resampled_df = iqr_removal(cotton_resampled_df), iqr_removal(rice_resampled_df)
combined_df = pd.concat([cotton_resampled_df, rice_resampled_df])
combined_df = shuffle(combined_df)
X, y = combined_df.drop('label', axis=1), combined_df['label']
model = train_model(X, y, ml_algo='bagging')


n_features = X.shape[1]
feature_importances = np.zeros(n_features)

for estimator, features_used in zip(model.estimators_, model.estimators_features_):
    tree_importances = estimator.feature_importances_

    for feature_idx, importance in zip(features_used, tree_importances):
        feature_importances[feature_idx] += importance

feature_importances /= np.sum(feature_importances)
sorted_indices = np.argsort(feature_importances)[::-1]

feature_importances = feature_importances[sorted_indices]


plt.figure(figsize=(10, 6))
plt.bar(X.columns[sorted_indices].tolist(), feature_importances, align='center')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.xlabel('Features')
plt.ylabel('Aggregated Feature Importance')
plt.title('Feature Importance in Bagging Model')

"""## XGBoost"""

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
cotton_resampled_df, rice_resampled_df = pd.concat(cotton_data), pd.concat(rice_data)
cotton_resampled_df, rice_resampled_df, augmentation_tags = augment(kwargs, cotton_resampled_df, rice_resampled_df, gclass_upsample_ratio=2)
cotton_resampled_df, rice_resampled_df = standardize(cotton_resampled_df, rice_resampled_df, 'minmax')
cotton_resampled_df, rice_resampled_df = iqr_removal(cotton_resampled_df), iqr_removal(rice_resampled_df)
combined_df = pd.concat([cotton_resampled_df, rice_resampled_df])
combined_df = shuffle(combined_df)
X, y = combined_df.drop('label', axis=1), combined_df['label']
model = train_model(X, y, ml_algo='xgboost')

xgb.plot_importance(model)

"""## SVM"""

cotton_data, rice_data = load_data_from_csv(cotton_datset_path, rice_datset_path)
cotton_resampled_df, rice_resampled_df = pd.concat(cotton_data), pd.concat(rice_data)
cotton_resampled_df, rice_resampled_df, augmentation_tags = augment(kwargs, cotton_resampled_df, rice_resampled_df, gclass_upsample_ratio=1)
cotton_resampled_df, rice_resampled_df = standardize(cotton_resampled_df, rice_resampled_df, 'minmax')
cotton_resampled_df, rice_resampled_df = iqr_removal(cotton_resampled_df), iqr_removal(rice_resampled_df)
combined_df = pd.concat([cotton_resampled_df, rice_resampled_df])
combined_df = shuffle(combined_df)
# combined_df = combined_df.sample(len(combined_df)//10)
X, y = combined_df.drop('label', axis=1), combined_df['label']
model = train_model(X, y, ml_algo='svc')

perm_importance = permutation_importance(model, X, y)

feature_names = X.columns
features = np.array(feature_names)

sorted_idx = perm_importance.importances_mean.argsort()
plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance")